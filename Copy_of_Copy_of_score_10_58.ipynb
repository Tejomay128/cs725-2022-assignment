{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tejomay128/cs725-2022-assignment/blob/main/Copy_of_Copy_of_score_10_58.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2N2S3KvdW3m",
        "outputId": "92c6609e-e8e3-477d-849d-1b5f5dd2d8a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from audioop import bias\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import math\n",
        "\n",
        "# The seed will be fixed to 42 for this assignment.\n",
        "np.random.seed(35)\n",
        "\n",
        "NUM_FEATS = 90\n",
        "\n",
        "\n",
        "class Net(object):\n",
        "    '''\n",
        "    '''\n",
        "\n",
        "    def __init__(self, num_layers, hidden_size):\n",
        "        '''\n",
        "        Initialize the neural network.\n",
        "        Create weights and biases.\n",
        "\n",
        "        Here, we have provided an example structure for the weights and biases.\n",
        "        It is a list of weight and bias matrices, in which, the\n",
        "        dimensions of weights and biases are (assuming 1 input layer, 2 hidden layers, and 1 output layer):\n",
        "        weights: [(NUM_FEATS, num_units), (num_units, num_units), (num_units, num_units), (num_units, 1)]\n",
        "        biases: [(num_units, 1), (num_units, 1), (num_units, 1), (num_units, 1)]\n",
        "\n",
        "        Please note that this is just an example.\n",
        "        You are free to modify or entirely ignore this initialization as per your need.\n",
        "        Also you can add more state-tracking variables that might be useful to compute\n",
        "        the gradients efficiently.\n",
        "\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "                num_layers : Number of HIDDEN layers.\n",
        "                num_units : Number of units in each Hidden layer.\n",
        "        '''\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.a_states = []\n",
        "        self.h_states = []\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(\n",
        "            hidden_size[:-1], hidden_size[1:])]\n",
        "        self.biases = [np.random.randn(y, 1) for y in hidden_size[1:]]\n",
        "\n",
        "        # for w in self.biases:\n",
        "        # \tprint(w.shape)\n",
        "        # self.biases = []\n",
        "        # self.weights = []\n",
        "        # for i in range(self.num_layers - 1):\n",
        "        # \tif i==0:\n",
        "        # \t\t# Input layer\n",
        "        # \t\tself.weights.append(np.random.uniform(-1, 1, size=(NUM_FEATS, self.hidden_size[i])))\n",
        "        # \telse:\n",
        "        # \t\t# Hidden layer\n",
        "        # \t\tself.weights.append(np.random.uniform(-1, 1, size=(self.hidden_size[i], self.hidden_size[i+1])))\n",
        "\n",
        "        # \tself.biases.append(np.random.uniform(-1, 1, size=(self.hidden_size[i], 1)))\n",
        "\n",
        "        # Output layer\n",
        "        # self.biases.append(np.random.uniform(-1, 1, size=(1, 1)))\n",
        "        # self.weights.append(np.random.uniform(-1, 1, size=(self.hidden_size[-1], 1)))\n",
        "\n",
        "    def relu(self, X):\n",
        "        return np.maximum(0, X)\n",
        "\n",
        "    def d_relu(self, h):\n",
        "        return (h > 0) * 1\n",
        "\n",
        "    def d_loss(self, y_hat, y):\n",
        "        return y_hat - y\n",
        "\n",
        "    def __call__(self, X):\n",
        "        '''\n",
        "        Forward propagate the input X through the network,\n",
        "        and return the output.\n",
        "\n",
        "        Note that for a classification task, the output layer should\n",
        "        be a softmax layer. So perform the computations accordingly\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "                X : Input to the network, numpy array of shape m x d\n",
        "        Returns\n",
        "        ----------\n",
        "                y : Output of the network, numpy array of shape m x 1\n",
        "        '''\n",
        "        self.a_states = []\n",
        "        self.h_states = []\n",
        "        out = X\n",
        "        self.a_states.append(X)\n",
        "        # self.h_states.append(X)\n",
        "        # print(self.num_layers)\n",
        "        h = 0\n",
        "        count = 0\n",
        "        for i in range(self.num_layers):\n",
        "            # print(f'weights = {self.weights[i].shape}  biases = {self.biases[i].shape}')\n",
        "            h = np.dot(out, self.weights[i].T) + self.biases[i].T\n",
        "            # print(\"in loop h:\", self.weights[i].T.shape)\n",
        "            out = self.relu(h)\n",
        "            self.a_states.append(out)\n",
        "            self.h_states.append(h)\n",
        "            count += 1\n",
        "        # print(\"count\", count)\n",
        "        # h = np.dot(out, self.weights[-1]) + self.biases[-1].T\n",
        "        # print(\"H_shape:\", h.shape)\n",
        "        return h\n",
        "\n",
        "    def backward(self, X, y, y_hat, lamda):\n",
        "        '''\n",
        "        Compute and return gradients loss with respect to weights and biases.\n",
        "        (dL/dW and dL/db)\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "                X : Input to the network, numpy array of shape m x d\n",
        "                y : Output of the network, numpy array of shape m x 1\n",
        "                lamda : Regularization parameter.\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "                del_W : derivative of loss w.r.t. all weight values (a list of matrices).\n",
        "                del_b : derivative of loss w.r.t. all bias values (a list of vectors).\n",
        "\n",
        "        Hint: You need to do a forward pass before performing backward pass.\n",
        "        '''\n",
        "        # d_W and d_b saves the accumulated gradients\n",
        "        d_W = [np.zeros(w.shape) for w in self.weights]\n",
        "        d_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        # derivative = self.d_loss(y_hat, y)\n",
        "        # y = y.values\n",
        "        for i, (xi, yi) in enumerate(zip(X, y)):\n",
        "            # print(y_hat[i][0],yi)\n",
        "            # print(\"backward\",y_hat[i].shape, yi)\n",
        "            derivative = (self.d_loss(y_hat[i], yi)).reshape(1, 1)\n",
        "            # print(i)\n",
        "            del_W = [np.zeros(w.shape) for w in self.weights]\n",
        "            del_b = [np.zeros(b.shape) for b in self.biases]\n",
        "\n",
        "            del_b[-1] = derivative\n",
        "            del_W[-1] = np.dot(self.a_states[-2][i].reshape(-1, 1), derivative).T\n",
        "            # print(\"delw shape:\", del_W[-1].shape)\n",
        "            for j in range(2, self.num_layers + 1):\n",
        "                h = self.h_states[-j][i]\n",
        "                relu_dash = self.d_relu(h)\n",
        "                # print(relu_dash.shape, derivative.shape, self.weights[-j+1].shape)\n",
        "                derivative = np.dot(self.weights[-j + 1].T, derivative) * relu_dash.reshape(-1, 1)\n",
        "                del_b[-j] = derivative\n",
        "                # print(j, self.a_states[-4])\n",
        "                del_W[-j] = np.dot(derivative, self.a_states[-j - 1][i].reshape(1, -1))\n",
        "                # print(\"delw shape:\", j, del_W[-j].shape)\n",
        "\n",
        "            d_W = [dw + delw for dw, delw in zip(d_W, del_W)]\n",
        "            d_b = [db + delb for db, delb in zip(d_b, del_b)]\n",
        "            d_W = [(dw + 2 * lamda * w) / X.shape[0] for w, dw in zip(self.weights, d_W)]\n",
        "            d_b = [(db + 2 * lamda * b) / X.shape[0] for b, db in zip(self.biases, d_b)]\n",
        "            # instead of accumulating, can create a list of all derivative of weights\n",
        "        return d_W, d_b\n",
        "\n",
        "\n",
        "class Optimizer(object):\n",
        "    '''\n",
        "    '''\n",
        "\n",
        "    def __init__(self, learning_rate, net, gamma=0.0, alpha=1):\n",
        "        '''\n",
        "        Create a Gradient Descent based optimizer with given\n",
        "        learning rate.\n",
        "\n",
        "        Other parameters can also be passed to create different types of\n",
        "        optimizers.\n",
        "\n",
        "        Hint: You can use the class members to track various states of the\n",
        "        optimizer.\n",
        "        '''\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_state = [np.zeros(w.shape) for w in net.weights]\n",
        "        self.bias_state = [np.zeros(b.shape) for b in net.biases]\n",
        "\n",
        "    def step(self, weights, biases, delta_weights, delta_biases, lamda, t, epoch):\n",
        "        '''\n",
        "        Parameters\n",
        "        ----------\n",
        "                weights: Current weights of the network.\n",
        "                biases: Current biases of the network.\n",
        "                delta_weights: Gradients of weights with respect to loss.\n",
        "                delta_biases: Gradients of biases with respect to loss.\n",
        "                To do: RMS prop, momentum, ADAM,adagrad, adaboost\n",
        "        '''\n",
        "        # right now this function receives sum of all gradients\n",
        "        eps = 1e-9\n",
        "        # right now this function receives sum of all gradients\n",
        "        # self.weight_state = [self.gamma * sw + (1 - self.gamma) * (dw ** 2) for sw, dw in\n",
        "        #                      zip(self.weight_state, delta_weights)]\n",
        "        # self.bias_state = [self.gamma * b + (1 - self.gamma) * (db ** 2) for b, db in\n",
        "        #                    zip(self.bias_state, delta_biases)]\n",
        "        weights = [w -\n",
        "                   self.learning_rate * dw for w, dw in\n",
        "                   zip(weights, delta_weights)]\n",
        "        biases = [b -\n",
        "                  self.learning_rate * db for b, db in\n",
        "                  zip(biases, delta_biases)]\n",
        "        # weights = [w-lamda/t * w for w in weights]\n",
        "        # if epoch % 25 == 0:\n",
        "        #     self.learning_rate *= 0.95\n",
        "        '''weights = [(1 - self.learning_rate * lamda) * w -\n",
        "                   self.learning_rate * dw for w, dw in zip(weights, delta_weights)]\n",
        "        biases = [(1 - self.learning_rate * lamda) * b -\n",
        "                  self.learning_rate * db for b, db in zip(biases, delta_biases)]\n",
        "        return weights, biases'''\n",
        "        return weights, biases\n",
        "\n",
        "\n",
        "def loss_mse(y, y_hat):\n",
        "    '''\n",
        "    Compute Mean Squared Error (MSE) loss between ground-truth and predicted values.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "            y : targets, numpy array of shape m x 1\n",
        "            y_hat : predictions, numpy array of shape m x 1\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "            MSE loss between y and y_hat.\n",
        "    '''\n",
        "    return np.mean((y_hat - y) ** 2)\n",
        "\n",
        "\n",
        "def loss_regularization(weights, biases):\n",
        "    '''\n",
        "    Compute l2 regularization loss.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "            weights and biases of the network.\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "            l2 regularization loss\n",
        "    '''\n",
        "    sum = 0\n",
        "    for w in weights:\n",
        "        sum += np.sum(w ** 2)\n",
        "    for b in biases:\n",
        "        sum += np.sum(b ** 2)\n",
        "    # print(f'Sum = {sum}')\n",
        "    return sum\n",
        "\n",
        "\n",
        "def loss_fn(y, y_hat, weights, biases, lamda):\n",
        "    '''\n",
        "    Compute loss =  loss_mse(..) + lamda * loss_regularization(..)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "            y : targets, numpy array of shape m x 1\n",
        "            y_hat : predictions, numpy array of shape m x 1\n",
        "            weights and biases of the network\n",
        "            lamda: Regularization parameter\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "            l2 regularization loss\n",
        "    '''\n",
        "    # print(loss_mse(y, y_hat))\n",
        "    return loss_mse(y, y_hat) + lamda * loss_regularization(weights, biases)\n",
        "\n",
        "\n",
        "def rmse(y, y_hat):\n",
        "    '''\n",
        "    Compute Root Mean Squared Error (RMSE) loss between ground-truth and predicted values.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "            y : targets, numpy array of shape m x 1\n",
        "            y_hat : predictions, numpy array of shape m x 1\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "            RMSE between y and y_hat.\n",
        "    '''\n",
        "    rmse_loss = np.sqrt(loss_mse(y, y_hat))\n",
        "    return rmse_loss\n",
        "\n",
        "\n",
        "def cross_entropy_loss(y, y_hat):\n",
        "    '''\n",
        "    Compute cross entropy loss\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "            y : targets, numpy array of shape m x 1\n",
        "            y_hat : predictions, numpy array of shape m x 1\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "            cross entropy loss\n",
        "    '''\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "def train(\n",
        "        net, optimizer, lamda, batch_size, max_epochs,\n",
        "        train_input, train_target,\n",
        "        dev_input, dev_target\n",
        "):\n",
        "    '''\n",
        "        In this function, you will perform following steps:\n",
        "            1. Run gradient descent algorithm for `max_epochs` epochs.\n",
        "            2. For each bach of the training data\n",
        "                1.1 Compute gradients\n",
        "                1.2 Update weights and biases using step() of optimizer.\n",
        "            3. Compute RMSE on dev data after running `max_epochs` epochs.\n",
        "\n",
        "        Here we have added the code to loop over batches and perform backward pass\n",
        "        for each batch in the loop.\n",
        "        For this code also, you are free to heavily modify it.\n",
        "        '''\n",
        "    m = train_input.shape[0]\n",
        "    history_dev_rmse = []\n",
        "    history_epoch = []\n",
        "    history_train_rmse = []\n",
        "    tolerance = 0\n",
        "    for e in range(max_epochs):\n",
        "        epoch_loss = 0.\n",
        "        step = 1\n",
        "        history_epoch.append(e)\n",
        "        for i in range(0, m, batch_size):\n",
        "            # print(\"Step=\", i)\n",
        "            batch_input = train_input[i:i + batch_size]\n",
        "            # print(\"shape\", batch_input.shape)\n",
        "            batch_target = train_target[i:i + batch_size]\n",
        "            # print(\"shape\", batch_target.shape)\n",
        "            pred = net(batch_input)\n",
        "            # print(\"Pred_shape\", pred.shape)\n",
        "            # batch_target = batch_target.reshape(-1,1)\n",
        "            # print(pred.shape)\n",
        "            # print(batch_target.shape)\n",
        "            # Compute gradients of loss w.r.t. weights and biases\n",
        "            dW, db = net.backward(batch_input, batch_target, pred, lamda)\n",
        "            # print('backweights',dW)\n",
        "            # print('backbiases',db)\n",
        "\n",
        "            # Get updated weights based on current weights and gradients\n",
        "            weights_updated, biases_updated = optimizer.step(\n",
        "                net.weights, net.biases, dW, db, lamda, batch_size, e + 1)\n",
        "\n",
        "            # Update model's weights and biases\n",
        "            net.weights = weights_updated\n",
        "            net.biases = biases_updated\n",
        "            # print('weights',np.array(net.weights))\n",
        "            # print('biases',np.array(net.biases))\n",
        "\n",
        "            # Compute loss for the batch\n",
        "            batch_loss = loss_fn(batch_target, pred,\n",
        "                                 net.weights, net.biases, lamda)\n",
        "            epoch_loss += batch_loss\n",
        "\n",
        "            if step % 50 == 0:\n",
        "                # print(e,step,batch_loss.value)\n",
        "                print(f'Epochs:{e + 1}\\tstep:{step}\\tbatch_loss:{batch_loss:.4f}')\n",
        "                # print('Epochs: '+e+' Step: '+step+' batch_loss: '+batch_loss)\n",
        "\n",
        "            step += 1\n",
        "        print(f'Epoch loss: {epoch_loss}')\n",
        "        if (e + 1) % 30 == 0:\n",
        "            optimizer.learning_rate *= 0.95\n",
        "\n",
        "        # Write any early stopping conditions required (only for Part 2)\n",
        "        # Hint: You can also compute dev_rmse here and use it in the early\n",
        "        # \t\tstopping condition.\n",
        "\n",
        "        # After running `max_epochs` (for Part 1) epochs OR early stopping (for Part 2), compute the RMSE on dev data.\n",
        "        dev_pred = net(dev_input)\n",
        "        train_pred = net(train_input)\n",
        "\n",
        "        dev_rmse = rmse(dev_target.reshape(-1, 1), dev_pred)\n",
        "        # print(train_target.shape)\n",
        "        train_rmse = rmse(train_target.reshape(-1, 1), train_pred)\n",
        "        history_train_rmse.append(train_rmse)\n",
        "        history_dev_rmse.append(dev_rmse)\n",
        "\n",
        "        print('RMSE on dev data: {:.5f}'.format(dev_rmse))\n",
        "        print('RMSE on train data: {:.5f}'.format(train_rmse))\n",
        "        if len(history_dev_rmse) > 1 and history_dev_rmse[e] > history_dev_rmse[e - 1]:\n",
        "            tolerance += 1\n",
        "        else:\n",
        "            tolerance = 0\n",
        "        if tolerance > 4:\n",
        "            break\n",
        "    # print('len of train',len(history_train_rmse))\n",
        "    # print('len of train',len(history_dev_rmse))\n",
        "    xpoint = np.array(history_epoch)\n",
        "    ypoint = np.array(history_train_rmse)\n",
        "    plt.xlabel(\"epoch\")\n",
        "    zpoint = np.array(history_dev_rmse)\n",
        "    plt.title(f'dev vs rmse with rmsprop alpha = {optimizer.alpha},gamma = {optimizer.gamma}')\n",
        "    plt.plot(xpoint, ypoint, color='g', label='train_rmse')\n",
        "    plt.plot(xpoint, zpoint, color='r', label='dev_rmse')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_test_data_predictions(net, inputs, target):\n",
        "    '''\n",
        "    Perform forward pass on test data and get the final predictions that can\n",
        "    be submitted on Kaggle.\n",
        "    Write the final predictions to the part2.csv file.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        net : trained neural network\n",
        "        inputs : test input, numpy array of shape m x d\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "        predictions (optional): Predictions obtained from forward pass\n",
        "                                on test data, numpy array of shape m x 1\n",
        "    '''\n",
        "    pred = net(inputs)\n",
        "    # pred = (pred * (2011 - 1922)) + 1922\n",
        "    # target = (target * (2011 - 1922)) + 1922\n",
        "    # print(np.max(pred), np.min(pred))\n",
        "    history_prediction = []\n",
        "    history_target = []\n",
        "    history_count = []\n",
        "    correct_pred = 0\n",
        "    wrong_pred = 0\n",
        "    f = open('22d0367_2.csv', \"w\", newline='')\n",
        "    writer = csv.writer(f)\n",
        "    init_list = [\"Id\", \"Predictions\"]\n",
        "    writer.writerow(init_list)\n",
        "    for i, p in enumerate(pred):\n",
        "        write_list = [i + 1, p[0]]\n",
        "        writer.writerow(write_list)\n",
        "        history_prediction.append(p)\n",
        "        # history_target.append(t)\n",
        "        # history_count.append(i)\n",
        "        # if abs(int(float(p[0])) - int(t)) < 6:\n",
        "        #     correct_pred = correct_pred + 1\n",
        "        # else:\n",
        "        #     wrong_pred = wrong_pred + 1\n",
        "\n",
        "        # if i > 100:\n",
        "        # break\n",
        "    f.close()\n",
        "    # accuracy = (correct_pred / (correct_pred + wrong_pred)) * 100\n",
        "    # print('accuracy', accuracy)\n",
        "    # xpoint = np.array(history_count)\n",
        "    # ypoint = np.array(history_prediction)\n",
        "    # zpoint = np.array(history_target)\n",
        "    # plt.plot(xpoint, ypoint, color='g', label='pred')\n",
        "    # plt.plot(xpoint, zpoint, color='r', label='target')\n",
        "    # plt.legend()\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "def min_max_scaler(x):\n",
        "    # x_min = np.min(x, axis=0)\n",
        "    # x_max = np.max(x, axis=0)\n",
        "    # x = np.divide((x - x_min), (x_max - x_min))\n",
        "    x_mean = np.mean(x, axis=0)\n",
        "    x_var = np.var(x, axis=0)\n",
        "    x = (x - x_mean) / x_var\n",
        "    return x\n",
        "\n",
        "def read_data():\n",
        "    '''\n",
        "    Read the train, dev, and test datasets\n",
        "    '''\n",
        "    TRAIN_FILE = \"/content/drive/My Drive/SEM1/foundations of ML/ml_project/train_selected_features.csv\"\n",
        "    DEV_FILE = \"/content/drive/My Drive/SEM1/foundations of ML/ml_project/dev_selected_features.csv\"\n",
        "    TEST_FILE = \"/content/drive/My Drive/SEM1/foundations of ML/ml_project/test_selected_features.csv\"\n",
        "    TEST_FILE2 = \"/content/drive/My Drive/SEM1/foundations of ML/ml_project/test2.csv\"\n",
        "    train_df = pd.read_csv(TRAIN_FILE)\n",
        "    test_df = pd.read_csv(TEST_FILE)\n",
        "    test_df2 = pd.read_csv(TEST_FILE2)\n",
        "    dev_df = pd.read_csv(DEV_FILE)\n",
        "\n",
        "    temp = train_df.to_numpy()\n",
        "    np.random.shuffle(temp)\n",
        "    train_input = min_max_scaler(temp[:, 1:])\n",
        "    # train_input = min_max_scaler(train_df.iloc[:, 1:])\n",
        "    # train_target = min_max_scaler(train_df[train_df.columns[0]])\n",
        "    # train_target = np.divide((train_df.iloc[:, :1] - 1922), (2011 - 1922))\n",
        "    train_target = temp[:, 0]\n",
        "    dev_input = min_max_scaler(dev_df.to_numpy()[:, 1:])\n",
        "    # dev_input = min_max_scaler(dev_df.iloc[:, 1:])\n",
        "    # dev_target = min_max_scaler(dev_df[dev_df.columns[0]])\n",
        "    # dev_target = np.divide((dev_df.iloc[:, 1] - 1922), (2011 - 1922))\n",
        "    dev_target = dev_df.to_numpy()[:, 0]\n",
        "    # test_input = min_max_scaler(test_df.to_numpy())\n",
        "    # test_input = min_max_scaler(test_df.iloc[:, :])\n",
        "    # dev_target = dev_df[dev_df.columns[0]]\n",
        "    test_input = min_max_scaler(test_df.to_numpy())\n",
        "    test_input2 = min_max_scaler(test_df2.to_numpy())\n",
        "    # print(train_input.shape)\n",
        "    # print(train_target.shape)\n",
        "    # print(train_input.shape,train_target.shape)\n",
        "    return train_input, train_target, dev_input, dev_target, test_input, test_input2\t\t\n",
        "\n",
        "def main():\n",
        "    # Hyper-parameters\n",
        "    max_epochs = 500\n",
        "    batch_size = 64\n",
        "    learning_rate = 0.0003\n",
        "    num_layers = 3\n",
        "    hidden_size = [67, 34, 34, 1]\n",
        "    lamda = 1.0  # Regularization Parameter\n",
        "    gamma = 0.95\n",
        "    alpha = 0.001\n",
        "\n",
        "    train_input, train_target, dev_input, dev_target, test_input, test_input2 = read_data()\n",
        "    net = Net(num_layers, hidden_size)\n",
        "    optimizer = Optimizer(learning_rate, net, gamma, alpha)\n",
        "    train(\n",
        "        net, optimizer, lamda, batch_size, max_epochs,\n",
        "        train_input, train_target,\n",
        "        dev_input, dev_target\n",
        "    )\n",
        "    get_test_data_predictions(net, test_input, dev_target)\n",
        "    # print(dev_target)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sDDoiYeQBdaE",
        "outputId": "cc563c82-c651-4d70-a9f0-29a4bc7481ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs:1\tstep:50\tbatch_loss:7062.3491\n",
            "Epochs:1\tstep:100\tbatch_loss:6338.8622\n",
            "Epochs:1\tstep:150\tbatch_loss:5656.9606\n",
            "Epochs:1\tstep:200\tbatch_loss:5094.1373\n",
            "Epochs:1\tstep:250\tbatch_loss:6413.8987\n",
            "Epochs:1\tstep:300\tbatch_loss:5498.6660\n",
            "Epochs:1\tstep:350\tbatch_loss:4845.4043\n",
            "Epochs:1\tstep:400\tbatch_loss:5015.1708\n",
            "Epochs:1\tstep:450\tbatch_loss:4995.4277\n",
            "Epochs:1\tstep:500\tbatch_loss:4760.2100\n",
            "Epochs:1\tstep:550\tbatch_loss:4845.9477\n",
            "Epochs:1\tstep:600\tbatch_loss:5192.2771\n",
            "Epoch loss: 63519550.227521695\n",
            "RMSE on dev data: 36.25606\n",
            "RMSE on train data: 36.87144\n",
            "Epochs:2\tstep:50\tbatch_loss:5005.6580\n",
            "Epochs:2\tstep:100\tbatch_loss:4749.4420\n",
            "Epochs:2\tstep:150\tbatch_loss:4742.6763\n",
            "Epochs:2\tstep:200\tbatch_loss:4398.6596\n",
            "Epochs:2\tstep:250\tbatch_loss:5143.1996\n",
            "Epochs:2\tstep:300\tbatch_loss:4665.2224\n",
            "Epochs:2\tstep:350\tbatch_loss:4539.0173\n",
            "Epochs:2\tstep:400\tbatch_loss:4472.7226\n",
            "Epochs:2\tstep:450\tbatch_loss:4471.7910\n",
            "Epochs:2\tstep:500\tbatch_loss:4378.7209\n",
            "Epochs:2\tstep:550\tbatch_loss:4367.9822\n",
            "Epochs:2\tstep:600\tbatch_loss:4717.5832\n",
            "Epoch loss: 3018402.443605205\n",
            "RMSE on dev data: 30.39854\n",
            "RMSE on train data: 30.93674\n",
            "Epochs:3\tstep:50\tbatch_loss:4646.0002\n",
            "Epochs:3\tstep:100\tbatch_loss:4398.7064\n",
            "Epochs:3\tstep:150\tbatch_loss:4486.6381\n",
            "Epochs:3\tstep:200\tbatch_loss:4224.2750\n",
            "Epochs:3\tstep:250\tbatch_loss:4721.1579\n",
            "Epochs:3\tstep:300\tbatch_loss:4438.6089\n",
            "Epochs:3\tstep:350\tbatch_loss:4405.9271\n",
            "Epochs:3\tstep:400\tbatch_loss:4253.8108\n",
            "Epochs:3\tstep:450\tbatch_loss:4251.9487\n",
            "Epochs:3\tstep:500\tbatch_loss:4195.6572\n",
            "Epochs:3\tstep:550\tbatch_loss:4180.8387\n",
            "Epochs:3\tstep:600\tbatch_loss:4505.5556\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-2103e1dd720b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-2103e1dd720b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0mdev_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m     )\n\u001b[1;32m    523\u001b[0m     \u001b[0mget_test_data_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-2103e1dd720b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, optimizer, lamda, batch_size, max_epochs, train_input, train_target, dev_input, dev_target)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;31m# print(batch_target.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;31m# Compute gradients of loss w.r.t. weights and biases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0;31m# print('backweights',dW)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;31m# print('backbiases',db)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-2103e1dd720b>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, X, y, y_hat, lamda)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;31m# print(i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mdel_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mdel_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mdel_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mderivative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}